{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c544e5ee-c372-41ab-903e-2102b96d41c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "block_size = 9\n",
    "batch_size = 4\n",
    "max_iters = 10000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c04b556d-cdd3-42ac-9289-6d6976fec7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('shakespeare_sonnets.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5de4788a-f514-4778-bd25-0d03a0894254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29, 18, 15,  1, 28, 25, 24, 24, 15, 29, 28,  0,  0, 36, 59,  1, 32, 43,\n",
      "        46, 46, 43, 35, 47,  1, 28, 42, 35, 45, 39, 53, 50, 39, 35, 52, 39,  0,\n",
      "         0,  0,  0,  0, 19,  0,  0, 16, 52, 49, 47,  1, 40, 35, 43, 52, 39, 53,\n",
      "        54,  1, 37, 52, 39, 35, 54, 55, 52, 39, 53,  1, 57, 39,  1, 38, 39, 53,\n",
      "        43, 52, 39,  1, 43, 48, 37, 52, 39, 35, 53, 39,  5,  0, 29, 42, 35, 54,\n",
      "         1, 54, 42, 39, 52, 39, 36, 59,  1])\n"
     ]
    }
   ],
   "source": [
    "# Function for encoding each character in the book to an integer and decoding the same integer to the character\n",
    "str_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_str = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [str_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_str[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "535e1bd1-991a-4172-bd04-a412953d825b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[45, 39,  5,  0, 25, 52,  1, 59, 49],\n",
      "        [ 1, 35,  1, 46, 49, 49, 45,  5,  0],\n",
      "        [52,  1, 54, 42, 39, 39,  1, 57, 43],\n",
      "        [35, 47, 39, 48, 38, 53,  0, 16, 49]], device='cuda:0')\n",
      "Outputs:\n",
      "tensor([[39,  5,  0, 25, 52,  1, 59, 49, 55],\n",
      "        [35,  1, 46, 49, 49, 45,  5,  0, 25],\n",
      "        [ 1, 54, 42, 39, 39,  1, 57, 43, 46],\n",
      "        [47, 39, 48, 38, 53,  0, 16, 49, 52]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Splittign into train and validation\n",
    "n = int(0.8*len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "x,y = get_batch('train')\n",
    "print('inputs:')\n",
    "print(x)\n",
    "\n",
    "print('Outputs:')\n",
    "print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2560dccc-6262-401a-b290-794af2d34bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([64]) target is tensor(29)\n",
      "When input is tensor([64, 29]) target is tensor(18)\n",
      "When input is tensor([64, 29, 18]) target is tensor(15)\n",
      "When input is tensor([64, 29, 18, 15]) target is tensor(1)\n",
      "When input is tensor([64, 29, 18, 15,  1]) target is tensor(28)\n",
      "When input is tensor([64, 29, 18, 15,  1, 28]) target is tensor(25)\n",
      "When input is tensor([64, 29, 18, 15,  1, 28, 25]) target is tensor(24)\n",
      "When input is tensor([64, 29, 18, 15,  1, 28, 25, 24]) target is tensor(24)\n",
      "When input is tensor([64, 29, 18, 15,  1, 28, 25, 24, 24]) target is tensor(15)\n"
     ]
    }
   ],
   "source": [
    "# Sample on hoe Bigram model works\n",
    "x = train[:block_size]\n",
    "y = train[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print('When input is', context, 'target is', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1ca84f0-d235-44d6-aaea-efa0681f8b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logiots, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6dffe11a-483c-432e-acde-0ee1370fe148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HNKdri tV)rlooorkluuUnMjUYPbhXH?NY-FFWsB-Rolu)v?ftxSIpDh’hLaSdc cX.YR-jCFOcT(fT(JkfTGsqoACyrlVIY—BkqNfXfVNfF)pm\n",
      "xA;T(VYb WqEjT(J:—Hmm‘MA(Mc—:—wpJA(LYA W\n",
      "qWzp(A(wLeFPnlmzjU:q;RE’kJeDni;KOq)D-PqGrbfWYksT:FH﻿jUh St-RJ(Dnfy(AVR\n",
      "maUg!I(.Dk;J(nP;t’PhwdsqMPna,—!HwD,t-o—;up,IPm﻿SY,T—!z; ?(LgXsMKlcuaxl!YX SuVELH)epy﻿eCP,,UO.:—LwNBgT(.’:BIHj c!gL VB -:EI;YYDcfC‘EjwytuImgT—;K\n",
      "w’Ye.gqq’Yk—OJ)tFv,w-t.M?Hmbt:—;GsjIxx—;AKvW-Bk’zivpsT’rAecONk’oUc;ImiPYgdm﻿iyCy,\n",
      "HDnqr.dcUEvGFi,:YAjbPE\n",
      "‘t‘Cegyy(JX,sRe(ww ,i﻿thyEv\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Make it vocab_size*vocab_size as it will given the probability of each character apprearing after a particular charatcter\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        # B is batch, T is time dimension, C is channel/all the characters\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # cross_entropy expects the dimension to be N, C so we reshape it\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array o findices in the corrent contex\n",
    "        for _ in range(max_new_tokens):\n",
    "            # gte the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:,-1,:] # Becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # Sample from distribution\n",
    "            index_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim =-1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6725c162-7d64-41d1-bfd7-66b9b96c1323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train_loss: 2.382, val_loss: 2.394\n",
      "step: 250, train_loss: 2.379, val_loss: 2.378\n",
      "step: 500, train_loss: 2.402, val_loss: 2.397\n",
      "step: 750, train_loss: 2.388, val_loss: 2.393\n",
      "step: 1000, train_loss: 2.379, val_loss: 2.409\n",
      "step: 1250, train_loss: 2.377, val_loss: 2.426\n",
      "step: 1500, train_loss: 2.371, val_loss: 2.404\n",
      "step: 1750, train_loss: 2.366, val_loss: 2.419\n",
      "step: 2000, train_loss: 2.379, val_loss: 2.390\n",
      "step: 2250, train_loss: 2.367, val_loss: 2.407\n",
      "step: 2500, train_loss: 2.386, val_loss: 2.405\n",
      "step: 2750, train_loss: 2.387, val_loss: 2.417\n",
      "step: 3000, train_loss: 2.373, val_loss: 2.415\n",
      "step: 3250, train_loss: 2.386, val_loss: 2.393\n",
      "step: 3500, train_loss: 2.379, val_loss: 2.410\n",
      "step: 3750, train_loss: 2.392, val_loss: 2.412\n",
      "step: 4000, train_loss: 2.378, val_loss: 2.391\n",
      "step: 4250, train_loss: 2.368, val_loss: 2.391\n",
      "step: 4500, train_loss: 2.383, val_loss: 2.391\n",
      "step: 4750, train_loss: 2.361, val_loss: 2.388\n",
      "step: 5000, train_loss: 2.367, val_loss: 2.412\n",
      "step: 5250, train_loss: 2.369, val_loss: 2.410\n",
      "step: 5500, train_loss: 2.388, val_loss: 2.369\n",
      "step: 5750, train_loss: 2.380, val_loss: 2.386\n",
      "step: 6000, train_loss: 2.376, val_loss: 2.409\n",
      "step: 6250, train_loss: 2.379, val_loss: 2.375\n",
      "step: 6500, train_loss: 2.353, val_loss: 2.411\n",
      "step: 6750, train_loss: 2.386, val_loss: 2.407\n",
      "step: 7000, train_loss: 2.362, val_loss: 2.406\n",
      "step: 7250, train_loss: 2.375, val_loss: 2.393\n",
      "step: 7500, train_loss: 2.379, val_loss: 2.391\n",
      "step: 7750, train_loss: 2.380, val_loss: 2.401\n",
      "step: 8000, train_loss: 2.362, val_loss: 2.406\n",
      "step: 8250, train_loss: 2.369, val_loss: 2.403\n",
      "step: 8500, train_loss: 2.379, val_loss: 2.411\n",
      "step: 8750, train_loss: 2.354, val_loss: 2.410\n",
      "step: 9000, train_loss: 2.390, val_loss: 2.378\n",
      "step: 9250, train_loss: 2.364, val_loss: 2.397\n",
      "step: 9500, train_loss: 2.361, val_loss: 2.416\n",
      "step: 9750, train_loss: 2.374, val_loss: 2.386\n",
      "2.174750328063965\n"
     ]
    }
   ],
   "source": [
    "# Pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train_loss: {losses['train']:.3f}, val_loss: {losses['val']:.3f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9302e03f-8da5-44d4-8e89-fab603bd308f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Butinglinay t o’d throushofans besthewhatrixEy t d Thy’de thingha thy ieldot imbr igt rer qO helds Wh  wisthy nove y ith, me nd, ne ty,\n",
      "I ghothe’do te,  s artulyo my greak, spicie, mee.\n",
      "XI are aknd twhieeeancty beseng y  elith iche isw or thoveapos,   mar bryms n mep-puder  ave,\n",
      "Whyout\n",
      "Neamuthowaldy awef fot impspy oamesth byes,\n",
      "And’\n",
      "\n",
      " hyo.\n",
      " he’s tsd I bu acouce\n",
      "Thad s peysting th owinchonoreehe whaz:\n",
      "Thexmanfre he wr Tht’se:\n",
      "‘Jzid my, ar mer fldefee re,\n",
      " meayowit  Ahoow, biles,\n",
      "PYelerithecke,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c66db-3a5e-46fe-b409-91c1624f8584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
